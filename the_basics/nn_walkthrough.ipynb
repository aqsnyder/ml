{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network\n",
    "\n",
    "This notebook walks through the creation of a simple neural network with one input node, two hidden layers, and one output node.\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "- Input Layer:  1 node\n",
    "- Hidden Layer  1: 1 nodes (arbitrary choice for demonstration)\n",
    "- Hidden Layer  2: 1 nodes (arbitrary choice for demonstration)\n",
    "- Output Layer: 1 node\n",
    "\n",
    "We'll start by visualizing the network architecture using Graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.0.0 (20240704.0754)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"357pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 357.40 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-40 353.4,-40 353.4,4 -4,4\"/>\n",
       "<!-- I -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>I</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"30.11\" cy=\"-18\" rx=\"30.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"30.11\" y=\"-12.57\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Input</text>\n",
       "</g>\n",
       "<!-- H1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>H1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"123.21\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"123.21\" y=\"-12.57\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">H1</text>\n",
       "</g>\n",
       "<!-- I&#45;&gt;H1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>I&#45;&gt;H1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.69,-18C68.3,-18 76.59,-18 84.55,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.34,-21.5 94.34,-18 84.34,-14.5 84.34,-21.5\"/>\n",
       "</g>\n",
       "<!-- H2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>H2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"213.21\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.21\" y=\"-12.57\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">H2</text>\n",
       "</g>\n",
       "<!-- H1&#45;&gt;H2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>H1&#45;&gt;H2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M150.62,-18C158.1,-18 166.39,-18 174.41,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.31,-21.5 184.31,-18 174.31,-14.5 174.31,-21.5\"/>\n",
       "</g>\n",
       "<!-- O -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>O</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"312.81\" cy=\"-18\" rx=\"36.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.81\" y=\"-12.57\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Output</text>\n",
       "</g>\n",
       "<!-- H2&#45;&gt;O -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>H2&#45;&gt;O</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M240.62,-18C247.99,-18 256.22,-18 264.41,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"264.33,-21.5 274.33,-18 264.33,-14.5 264.33,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1b6f98274d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def visualize_nn():\n",
    "    dot = Digraph()\n",
    "    \n",
    "    # Set graph attributes for left-to-right orientation and size\n",
    "    dot.attr(rankdir='LR', size='10,5')\n",
    "    \n",
    "    # Input Layer\n",
    "    dot.node('I', 'Input')\n",
    "    \n",
    "    # Hidden Layer 1\n",
    "    dot.node('H1', 'H1')\n",
    "    \n",
    "    # Hidden Layer 2\n",
    "    dot.node('H2', 'H2')\n",
    "    \n",
    "    # Output Layer\n",
    "    dot.node('O', 'Output')\n",
    "    \n",
    "    # Edges\n",
    "    dot.edge('I', 'H1')\n",
    "    dot.edge('H1', 'H2')\n",
    "    dot.edge('H2', 'O')\n",
    "    \n",
    "    return dot\n",
    "\n",
    "visualize_nn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Implementation\n",
    "\n",
    "We'll implement the forward pass of the neural network, where we compute the output of the network layer by layer.\n",
    "\n",
    "#### Forward Pass Equations\n",
    "\n",
    "For our simplified network, the equations are:\n",
    "\n",
    "- **Input to Hidden Layer 1:**\n",
    "  $$\n",
    "  H1 = \\sigma(W1 \\cdot I + b1)\n",
    "  $$\n",
    "\n",
    "- **Hidden Layer 1 to Hidden Layer 2:**\n",
    "  $$\n",
    "  H2 = \\sigma(W2 \\cdot H1 + b2)\n",
    "  $$\n",
    "\n",
    "- **Hidden Layer 2 to Output:**\n",
    "  $$\n",
    "  O = \\sigma(W3 \\cdot H2 + b3)\n",
    "  $$\n",
    "\n",
    "Where:\n",
    "- $ \\sigma(x) $ is the activation function (we'll use the sigmoid function).\n",
    "- $ W1, W2, W3 $ are the weight matrices for each layer.\n",
    "- $ b1, b2, b3 $ are the bias terms for each layer.\n",
    "- $ I $ is the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the network: [[0.54406737]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function: Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid (for backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = 1\n",
    "hidden1_size = 1\n",
    "hidden2_size = 1\n",
    "output_size = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "W1 = np.random.rand(input_size, hidden1_size)\n",
    "b1 = np.random.rand(hidden1_size)\n",
    "W2 = np.random.rand(hidden1_size, hidden2_size)\n",
    "b2 = np.random.rand(hidden2_size)\n",
    "W3 = np.random.rand(hidden2_size, output_size)\n",
    "b3 = np.random.rand(output_size)\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(X):\n",
    "    H1 = sigmoid(np.dot(X, W1) + b1)\n",
    "    H2 = sigmoid(np.dot(H1, W2) + b2)\n",
    "    O = sigmoid(np.dot(H2, W3) + b3)\n",
    "    return O\n",
    "\n",
    "# Example input\n",
    "X = np.array([[0.5]])\n",
    "output = forward_pass(X)\n",
    "print(\"Output of the network:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation and Training\n",
    "\n",
    "We'll train the neural network using backpropagation. This involves calculating the loss, computing the gradient of the loss with respect to the weights, and updating the weights to minimize the loss.\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "We'll use the mean squared error (MSE) as our loss function:\n",
    "$$\n",
    "L = \\frac{1}{2} \\cdot (O_{\\text{pred}} - O_{\\text{true}})^2\n",
    "$$\n",
    "\n",
    "#### Backpropagation Algorithm\n",
    "\n",
    "We'll update the weights using the gradient descent method:\n",
    "\n",
    "1. **Compute the loss** between the predicted output and the true output.\n",
    "2. **Backpropagate** the loss through the network to compute the gradients.\n",
    "3. **Update** the weights using the gradients and a learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.10393728123644523\n",
      "Epoch 100, Loss: 0.028852247567119793\n",
      "Epoch 200, Loss: 0.014092016829799261\n",
      "Epoch 300, Loss: 0.008854840979798604\n",
      "Epoch 400, Loss: 0.006313773416746348\n",
      "Epoch 500, Loss: 0.004848288927054119\n",
      "Epoch 600, Loss: 0.003906962697985976\n",
      "Epoch 700, Loss: 0.003256408094114442\n",
      "Epoch 800, Loss: 0.0027824284778798624\n",
      "Epoch 900, Loss: 0.002423067017565997\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# True output for training\n",
    "y_true = np.array([[1]])\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    H1 = sigmoid(np.dot(X, W1) + b1)\n",
    "    H2 = sigmoid(np.dot(H1, W2) + b2)\n",
    "    O = sigmoid(np.dot(H2, W3) + b3)\n",
    "    \n",
    "    # Calculate the loss (MSE)\n",
    "    loss = 0.5 * (y_true - O) ** 2\n",
    "    \n",
    "    # Backpropagation\n",
    "    dO = (O - y_true) * sigmoid_derivative(O)\n",
    "    dH2 = np.dot(dO, W3.T) * sigmoid_derivative(H2)\n",
    "    dH1 = np.dot(dH2, W2.T) * sigmoid_derivative(H1)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W3 -= learning_rate * np.dot(H2.T, dO)\n",
    "    b3 -= learning_rate * np.sum(dO, axis=0)\n",
    "    W2 -= learning_rate * np.dot(H1.T, dH2)\n",
    "    b2 -= learning_rate * np.sum(dH2, axis=0)\n",
    "    W1 -= learning_rate * np.dot(X.T, dH1)\n",
    "    b1 -= learning_rate * np.sum(dH1, axis=0)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {np.sum(loss)}')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we've built a simple neural network with one input, two hidden layers (each with one node), and one output. We visualized the network structure, implemented the forward pass, and trained the network using backpropagation. This is a foundational example that can be extended to more complex networks with more layers and nodes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-qbbDuZAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
